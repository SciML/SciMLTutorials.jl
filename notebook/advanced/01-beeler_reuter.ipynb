{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Background\n\n[JuliaDiffEq](https://github.com/JuliaDiffEq) is a suite of optimized Julia libraries to solve ordinary differential equations (ODE). *JuliaDiffEq* provides a large number of explicit and implicit solvers suited for different types of ODE problems. It is possible to reduce a system of partial differential equations into an ODE problem by employing the [method of lines (MOL)](https://en.wikipedia.org/wiki/Method_of_lines). The essence of MOL is to discretize the spatial derivatives (by finite difference, finite volume or finite element methods) into algebraic equations and to keep the time derivatives as is. The resulting differential equations are left with only one independent variable (time) and can be solved with an ODE solver. [Solving Systems of Stochastic PDEs and using GPUs in Julia](http://www.stochasticlifestyle.com/solving-systems-stochastic-pdes-using-gpus-julia/) is a brief introduction to MOL and using GPUs to accelerate PDE solving in *JuliaDiffEq*. Here we expand on this introduction by developing an implicit/explicit (IMEX) solver for a 2D cardiac electrophysiology model and show how to use [CuArray](https://github.com/JuliaGPU/CuArrays.jl) and [CUDAnative](https://github.com/JuliaGPU/CUDAnative.jl) libraries to run the explicit part of the model on a GPU.\n\nNote that this tutorial does not use the [higher order IMEX methods built into DifferentialEquations.jl](http://docs.juliadiffeq.org/latest/solvers/split_ode_solve.html#Implicit-Explicit-(IMEX)-ODE-1) but instead shows how to hand-split an equation when the explicit portion has an analytical solution (or approxiate), which is common in many scenarios.\n\nThere are hundreds of ionic models that describe cardiac electrical activity in various degrees of detail. Most are based on the classic [Hodgkin-Huxley model](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model) and define the time-evolution of different state variables in the form of nonlinear first-order ODEs. The state vector for these models includes the transmembrane potential, gating variables, and ionic concentrations. The coupling between cells is through the transmembrame potential only and is described as a reaction-diffusion equation, which is a parabolic PDE,\n\n$$\\partial V / \\partial t = \\nabla (D  \\nabla V) - \\frac {I_\\text{ion}} {C_m},$$\n\nwhere $V$ is the transmembrane potential, $D$ is a diffusion tensor, $I_\\text{ion}$ is the sum of the transmembrane currents and is calculated from the ODEs, and $C_m$ is the membrane capacitance and is usually assumed to be constant. Here we model a uniform and isotropic medium. Therefore, the model can be simplified to,\n\n$$\\partial V / \\partial t = D \\Delta{V} - \\frac {I_\\text{ion}} {C_m},$$\n\nwhere $D$ is now a scalar. By nature, these models have to deal with different time scales and are therefore classified as *stiff*. Commonly, they are solved using the explicit Euler method, usually with a closed form for the integration of the gating variables (the Rush-Larsen method, see below). We can also solve these problems using implicit or semi-implicit PDE solvers (e.g., the [Crank-Nicholson method](https://en.wikipedia.org/wiki/Crank%E2%80%93Nicolson_method) combined with an iterative solver). Higher order explicit methods such as Runge-Kutta and linear multi-step methods cannot overcome the stiffness and are not particularly helpful.\n\nIn this tutorial, we first develop a CPU-only IMEX solver and then show how to move the explicit part to a GPU.\n\n### The Beeler-Reuter Model\n\nWe have chosen the [Beeler-Reuter ventricular ionic model](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1283659/) as our example. It is a classic model first described in 1977 and is used as a base for many other ionic models. It has eight state variables, which makes it complicated enough to be interesting without obscuring the main points of the exercise. The eight state variables are: the transmembrane potential ($V$), sodium-channel activation and inactivation gates ($m$ and $h$, similar to the Hodgkin-Huxley model), with an additional slow inactivation gate ($j$), calcium-channel activation and deactivations gates ($d$ and $f$), a time-dependent inward-rectifying potassium current gate ($x_1$), and intracellular calcium concentration ($c$). There are four currents: a sodium current ($i_{Na}$), a calcium current ($i_{Ca}$), and two potassium currents, one time-dependent ($i_{x_1}$) and one background time-independent ($i_{K_1}$).\n\n## CPU-Only Beeler-Reuter Solver\n\nLet's start by developing a CPU only IMEX solver. The main idea is to use the *DifferentialEquations* framework to handle the implicit part of the equation and code the analytical approximation for explicit part separately. If no analytical approximation was known for the explicit part, one could use methods from [this list](http://docs.juliadiffeq.org/latest/solvers/split_ode_solve.html#Implicit-Explicit-(IMEX)-ODE-1).\n\nFirst, we define the model constants:\n# An Implicit&#x2F;Explicit CUDA-Accelerated Solver for the 2D Beeler-Reuter Model\n### Shahriar Iravanian"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "const v0 = -84.624\nconst v1 = 10.0\nconst C_K1 = 1.0f0\nconst C_x1 = 1.0f0\nconst C_Na = 1.0f0\nconst C_s = 1.0f0\nconst D_Ca = 0.0f0\nconst D_Na = 0.0f0\nconst g_s = 0.09f0\nconst g_Na = 4.0f0\nconst g_NaC = 0.005f0\nconst ENa = 50.0f0 + D_Na\nconst γ = 0.5f0\nconst C_m = 1.0f0"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the constants are defined as `Float32` and not `Float64`. The reason is that most GPUs have many more single precision cores than double precision ones. To ensure uniformity between CPU and GPU, we also code most states variables as `Float32` except for the transmembrane potential, which is solved by an implicit solver provided by the Sundial library and needs to be `Float64`.\n\n### The State Structure\n\nNext, we define a struct to contain our state. `BeelerReuterCpu` is a functor and we will define a deriv function as its associated function."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "mutable struct BeelerReuterCpu <: Function\n    t::Float64              # the last timestep time to calculate Δt\n    diff_coef::Float64      # the diffusion-coefficient (coupling strength)\n\n    C::Array{Float32, 2}    # intracellular calcium concentration\n    M::Array{Float32, 2}    # sodium current activation gate (m)\n    H::Array{Float32, 2}    # sodium current inactivation gate (h)\n    J::Array{Float32, 2}    # sodium current slow inactivaiton gate (j)\n    D::Array{Float32, 2}    # calcium current activaiton gate (d)\n    F::Array{Float32, 2}    # calcium current inactivation gate (f)\n    XI::Array{Float32, 2}   # inward-rectifying potassium current (iK1)\n\n    Δu::Array{Float64, 2}   # place-holder for the Laplacian\n\n    function BeelerReuterCpu(u0, diff_coef)\n        self = new()\n\n        ny, nx = size(u0)\n        self.t = 0.0\n        self.diff_coef = diff_coef\n\n        self.C = fill(0.0001f0, (ny,nx))\n        self.M = fill(0.01f0, (ny,nx))\n        self.H = fill(0.988f0, (ny,nx))\n        self.J = fill(0.975f0, (ny,nx))\n        self.D = fill(0.003f0, (ny,nx))\n        self.F = fill(0.994f0, (ny,nx))\n        self.XI = fill(0.0001f0, (ny,nx))\n\n        self.Δu = zeros(ny,nx)\n\n        return self\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Laplacian\n\nThe finite-difference Laplacian is calculated in-place by a 5-point stencil. The Neumann boundary condition is enforced. Note that we could have also used [DiffEqOperators.jl](https://github.com/JuliaDiffEq/DiffEqOperators.jl) to automate this step."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# 5-point stencil\nfunction laplacian(Δu, u)\n    n1, n2 = size(u)\n\n    # internal nodes\n    for j = 2:n2-1\n        for i = 2:n1-1\n            @inbounds  Δu[i,j] = u[i+1,j] + u[i-1,j] + u[i,j+1] + u[i,j-1] - 4*u[i,j]\n        end\n    end\n\n    # left/right edges\n    for i = 2:n1-1\n        @inbounds Δu[i,1] = u[i+1,1] + u[i-1,1] + 2*u[i,2] - 4*u[i,1]\n        @inbounds Δu[i,n2] = u[i+1,n2] + u[i-1,n2] + 2*u[i,n2-1] - 4*u[i,n2]\n    end\n\n    # top/bottom edges\n    for j = 2:n2-1\n        @inbounds Δu[1,j] = u[1,j+1] + u[1,j-1] + 2*u[2,j] - 4*u[1,j]\n        @inbounds Δu[n1,j] = u[n1,j+1] + u[n1,j-1] + 2*u[n1-1,j] - 4*u[n1,j]\n    end\n\n    # corners\n    @inbounds Δu[1,1] = 2*(u[2,1] + u[1,2]) - 4*u[1,1]\n    @inbounds Δu[n1,1] = 2*(u[n1-1,1] + u[n1,2]) - 4*u[n1,1]\n    @inbounds Δu[1,n2] = 2*(u[2,n2] + u[1,n2-1]) - 4*u[1,n2]\n    @inbounds Δu[n1,n2] = 2*(u[n1-1,n2] + u[n1,n2-1]) - 4*u[n1,n2]\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Rush-Larsen Method\n\nWe use an explicit solver for all the state variables except for the transmembrane potential which is solved with the help of an implicit solver. The explicit solver is a domain-specific exponential method, the Rush-Larsen method. This method utilizes an approximation on the model in order to transform the IMEX equation into a form suitable for an implicit ODE solver. This combination of implicit and explicit methods forms a specialized IMEX solver. For general IMEX integration, please see the [IMEX solvers documentation](http://docs.juliadiffeq.org/latest/solvers/split_ode_solve.html#Implicit-Explicit-%28IMEX%29-ODE-1). While we could have used the general model to solve the current problem, for this specific model, the transformation approach is more efficient and is of practical interest.\n\nThe [Rush-Larsen](https://ieeexplore.ieee.org/document/4122859/) method replaces the explicit Euler integration for the gating variables with direct integration. The starting point is the general ODE for the gating variables in Hodgkin-Huxley style ODEs,\n\n$$\\frac{dg}{dt} = \\alpha(V) (1 - g) - \\beta(V) g$$\n\nwhere $g$ is a generic gating variable, ranging from 0 to 1, and $\\alpha$ and $\\beta$ are reaction rates. This equation can be written as,\n\n$$\\frac{dg}{dt} = (g_{\\infty} - g) / \\tau_g,$$\n\nwhere $g_\\infty$ and $\\tau_g$ are\n\n$$g_{\\infty} = \\frac{\\alpha}{(\\alpha + \\beta)},$$\n\nand,\n\n$$\\tau_g = \\frac{1}{(\\alpha + \\beta)}.$$\n\nAssuing that $g_\\infty$ and $\\tau_g$ are constant for the duration of a single time step ($\\Delta{t}$), which is a reasonable assumption for most cardiac models, we can integrate directly to have,\n\n$$g(t + \\Delta{t}) = g_{\\infty} - \\left(g_{\\infty} - g(\\Delta{t})\\right)\\,e^{-\\Delta{t}/\\tau_g}.$$\n\nThis is the Rush-Larsen technique. Note that as $\\Delta{t} \\rightarrow 0$, this equations morphs into the explicit Euler formula,\n\n$$g(t + \\Delta{t}) = g(t) + \\Delta{t}\\frac{dg}{dt}.$$\n\n`rush_larsen` is a helper function that use the Rush-Larsen method to integrate the gating variables."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@inline function rush_larsen(g, α, β, Δt)\n    inf = α/(α+β)\n    τ = 1f0 / (α+β)\n    return clamp(g + (g - inf) * expm1(-Δt/τ), 0f0, 1f0)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gating variables are updated as below. The details of how to calculate $\\alpha$ and $\\beta$ are based on the Beeler-Reuter model and not of direct interest to this tutorial."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function update_M_cpu(g, v, Δt)\n    # the condition is needed here to prevent NaN when v == 47.0\n    α = isapprox(v, 47.0f0) ? 10.0f0 : -(v+47.0f0) / (exp(-0.1f0*(v+47.0f0)) - 1.0f0)\n    β = (40.0f0 * exp(-0.056f0*(v+72.0f0)))\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_H_cpu(g, v, Δt)\n    α = 0.126f0 * exp(-0.25f0*(v+77.0f0))\n    β = 1.7f0 / (exp(-0.082f0*(v+22.5f0)) + 1.0f0)\n   return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_J_cpu(g, v, Δt)\n    α = (0.55f0 * exp(-0.25f0*(v+78.0f0))) / (exp(-0.2f0*(v+78.0f0)) + 1.0f0)\n    β = 0.3f0 / (exp(-0.1f0*(v+32.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_D_cpu(g, v, Δt)\n    α = γ * (0.095f0 * exp(-0.01f0*(v-5.0f0))) / (exp(-0.072f0*(v-5.0f0)) + 1.0f0)\n    β = γ * (0.07f0 * exp(-0.017f0*(v+44.0f0))) / (exp(0.05f0*(v+44.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_F_cpu(g, v, Δt)\n    α = γ * (0.012f0 * exp(-0.008f0*(v+28.0f0))) / (exp(0.15f0*(v+28.0f0)) + 1.0f0)\n    β = γ * (0.0065f0 * exp(-0.02f0*(v+30.0f0))) / (exp(-0.2f0*(v+30.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend\n\nfunction update_XI_cpu(g, v, Δt)\n    α = (0.0005f0 * exp(0.083f0*(v+50.0f0))) / (exp(0.057f0*(v+50.0f0)) + 1.0f0)\n    β = (0.0013f0 * exp(-0.06f0*(v+20.0f0))) / (exp(-0.04f0*(v+20.0f0)) + 1.0f0)\n    return rush_larsen(g, α, β, Δt)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intracelleular calcium is not technically a gating variable, but we can use a similar explicit exponential integrator for it."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function update_C_cpu(g, d, f, v, Δt)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * log(g)\n    kCa = C_s * g_s * d * f\n    iCa = kCa * (v - ECa)\n    inf = 1.0f-7 * (0.07f0 - g)\n    τ = 1f0 / 0.07f0\n    return g + (g - inf) * expm1(-Δt/τ)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implicit Solver\n\nNow, it is time to define the derivative function as an associated function of **BeelerReuterCpu**. We plan to use the CVODE_BDF solver as our implicit portion. Similar to other iterative methods, it calls the deriv function with the same $t$ multiple times. For example, these are consecutive $t$s from a representative run:\n\n0.86830\n0.86830\n0.85485\n0.85485\n0.85485\n0.86359\n0.86359\n0.86359\n0.87233\n0.87233\n0.87233\n0.88598\n...\n\nHere, every time step is called three times. We distinguish between two types of calls to the deriv function. When $t$ changes, the gating variables are updated by calling `update_gates_cpu`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function update_gates_cpu(u, XI, M, H, J, D, F, C, Δt)\n    let Δt = Float32(Δt)\n        n1, n2 = size(u)\n        for j = 1:n2\n            for i = 1:n1\n                v = Float32(u[i,j])\n\n                XI[i,j] = update_XI_cpu(XI[i,j], v, Δt)\n                M[i,j] = update_M_cpu(M[i,j], v, Δt)\n                H[i,j] = update_H_cpu(H[i,j], v, Δt)\n                J[i,j] = update_J_cpu(J[i,j], v, Δt)\n                D[i,j] = update_D_cpu(D[i,j], v, Δt)\n                F[i,j] = update_F_cpu(F[i,j], v, Δt)\n\n                C[i,j] = update_C_cpu(C[i,j], D[i,j], F[i,j], v, Δt)\n            end\n        end\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "On the other hand, du is updated at each time step, since it is independent of $\\Delta{t}$."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# iK1 is the inward-rectifying potassium current\nfunction calc_iK1(v)\n    ea = exp(0.04f0*(v+85f0))\n    eb = exp(0.08f0*(v+53f0))\n    ec = exp(0.04f0*(v+53f0))\n    ed = exp(-0.04f0*(v+23f0))\n    return 0.35f0 * (4f0*(ea-1f0)/(eb + ec)\n            + 0.2f0 * (isapprox(v, -23f0) ? 25f0 : (v+23f0) / (1f0-ed)))\nend\n\n# ix1 is the time-independent background potassium current\nfunction calc_ix1(v, xi)\n    ea = exp(0.04f0*(v+77f0))\n    eb = exp(0.04f0*(v+35f0))\n    return xi * 0.8f0 * (ea-1f0) / eb\nend\n\n# iNa is the sodium current (similar to the classic Hodgkin-Huxley model)\nfunction calc_iNa(v, m, h, j)\n    return C_Na * (g_Na * m^3 * h * j + g_NaC) * (v - ENa)\nend\n\n# iCa is the calcium current\nfunction calc_iCa(v, d, f, c)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * log(c)    # ECa is the calcium reversal potential\n    return C_s * g_s * d * f * (v - ECa)\nend\n\nfunction update_du_cpu(du, u, XI, M, H, J, D, F, C)\n    n1, n2 = size(u)\n\n    for j = 1:n2\n        for i = 1:n1\n            v = Float32(u[i,j])\n\n            # calculating individual currents\n            iK1 = calc_iK1(v)\n            ix1 = calc_ix1(v, XI[i,j])\n            iNa = calc_iNa(v, M[i,j], H[i,j], J[i,j])\n            iCa = calc_iCa(v, D[i,j], F[i,j], C[i,j])\n\n            # total current\n            I_sum = iK1 + ix1 + iNa + iCa\n\n            # the reaction part of the reaction-diffusion equation\n            du[i,j] = -I_sum / C_m\n        end\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we put everything together is our deriv function, which is a call on `BeelerReuterCpu`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function (f::BeelerReuterCpu)(du, u, p, t)\n    Δt = t - f.t\n\n    if Δt != 0 || t == 0\n        update_gates_cpu(u, f.XI, f.M, f.H, f.J, f.D, f.F, f.C, Δt)\n        f.t = t\n    end\n\n    laplacian(f.Δu, u)\n\n    # calculate the reaction portion\n    update_du_cpu(du, u, f.XI, f.M, f.H, f.J, f.D, f.F, f.C)\n\n    # ...add the diffusion portion\n    du .+= f.diff_coef .* f.Δu\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results\n\nTime to test! We need to define the starting transmembrane potential with the help of global constants **v0** and **v1**, which represent the resting and activated potentials."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "const N = 192;\nu0 = fill(v0, (N, N));\nu0[90:102,90:102] .= v1;   # a small square in the middle of the domain"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The initial condition is a small square in the middle of the domain."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nheatmap(u0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, the problem is defined:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DifferentialEquations, Sundials\n\nderiv_cpu = BeelerReuterCpu(u0, 1.0);\nprob = ODEProblem(deriv_cpu, u0, (0.0, 50.0));"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For stiff reaction-diffusion equations, CVODE_BDF from Sundial library is an excellent solver."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@time sol = solve(prob, CVODE_BDF(linear_solver=:GMRES), saveat=100.0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "heatmap(sol.u[end])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPU/GPU Beeler-Reuter Solver\n\nGPUs are great for embarrassingly parallel problems but not so much for highly coupled models. We plan to keep the implicit part on CPU and run the decoupled explicit code on a GPU with the help of the CUDAnative library.\n\n### GPUs and CUDA\n\nIt this section, we present a brief summary of how GPUs (specifically NVIDIA GPUs) work and how to program them using the Julia CUDA interface. The readers who are familiar with these basic concepts may skip this section.\n\nLet's start by looking at the hardware of a typical high-end GPU, GTX 1080. It has four Graphics Processing Clusters (equivalent to a discrete CPU), each harboring five Streaming Multiprocessor (similar to a CPU core). Each SM has 128 single-precision CUDA cores. Therefore, GTX 1080 has a total of 4 x 5 x 128 = 2560 CUDA cores. The maximum  theoretical throughput for a GTX 1080 is reported as 8.87 TFLOPS. This figure is calculated for a boost clock frequency of 1.733 MHz as 2 x 2560 x 1.733 MHz = 8.87 TFLOPS. The factor 2 is included because two single floating point operations, a multiplication and an addition, can be done in a clock cycle as part of a fused-multiply-addition FMA operation. GTX 1080 also has 8192 MB of global memory accessible to all the cores (in addition to local and shared memory on each SM).\n\nA typical CUDA application has the following flow:\n\n1. Define and initialize the problem domain tensors (multi-dimensional arrays) in CPU memory.\n2. Allocate corresponding tensors in the GPU global memory.\n3. Transfer the input tensors from CPU to the corresponding GPU tensors.\n4. Invoke CUDA kernels (i.e., the GPU functions callable from CPU) that operate on the GPU tensors.\n5. Transfer the result tensors from GPU back to CPU.\n6. Process tensors on CPU.\n7. Repeat steps 3-6 as needed.\n\nSome libraries, such as [ArrayFire](https://github.com/arrayfire/arrayfire), hide the complexicities of steps 2-5 behind a higher level of abstraction. However, here we take a lower level route. By using [CuArray](https://github.com/JuliaGPU/CuArrays.jl) and [CUDAnative](https://github.com/JuliaGPU/CUDAnative.jl), we achieve a finer-grained control and higher performance. In return, we need to implement each step manually.\n\n*CuArray* is a thin abstraction layer over the CUDA API and allows us to define GPU-side tensors and copy data to and from them but does not provide for operations on tensors. *CUDAnative* is a compiler that translates Julia functions designated as CUDA kernels into ptx (a high-level CUDA assembly language).\n\n### The CUDA Code\n\nThe key to fast CUDA programs is to minimize CPU/GPU memory transfers and global memory accesses. The implicit solver is currently CPU only, but it only needs access to the transmembrane potential. The rest of state variables reside on the GPU memory.\n\nWe modify ``BeelerReuterCpu`` into ``BeelerReuterGpu`` by defining the state variables as *CuArray*s instead of standard Julia *Array*s. The name of each variable defined on GPU is prefixed by *d_* for clarity. Note that $\\Delta{v}$ is a temporary storage for the Laplacian and stays on the CPU side."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using CUDAnative, CuArrays\n\nmutable struct BeelerReuterGpu <: Function\n    t::Float64                  # the last timestep time to calculate Δt\n    diff_coef::Float64          # the diffusion-coefficient (coupling strength)\n\n    d_C::CuArray{Float32, 2}    # intracellular calcium concentration\n    d_M::CuArray{Float32, 2}    # sodium current activation gate (m)\n    d_H::CuArray{Float32, 2}    # sodium current inactivation gate (h)\n    d_J::CuArray{Float32, 2}    # sodium current slow inactivaiton gate (j)\n    d_D::CuArray{Float32, 2}    # calcium current activaiton gate (d)\n    d_F::CuArray{Float32, 2}    # calcium current inactivation gate (f)\n    d_XI::CuArray{Float32, 2}   # inward-rectifying potassium current (iK1)\n\n    d_u::CuArray{Float64, 2}    # place-holder for u in the device memory\n    d_du::CuArray{Float64, 2}   # place-holder for d_u in the device memory\n\n    Δv::Array{Float64, 2}       # place-holder for voltage gradient\n\n    function BeelerReuterGpu(u0, diff_coef)\n        self = new()\n\n        ny, nx = size(u0)\n        @assert (nx % 16 == 0) && (ny % 16 == 0)\n        self.t = 0.0\n        self.diff_coef = diff_coef\n\n        self.d_C = CuArray(fill(0.0001f0, (ny,nx)))\n        self.d_M = CuArray(fill(0.01f0, (ny,nx)))\n        self.d_H = CuArray(fill(0.988f0, (ny,nx)))\n        self.d_J = CuArray(fill(0.975f0, (ny,nx)))\n        self.d_D = CuArray(fill(0.003f0, (ny,nx)))\n        self.d_F = CuArray(fill(0.994f0, (ny,nx)))\n        self.d_XI = CuArray(fill(0.0001f0, (ny,nx)))\n\n        self.d_u = CuArray(u0)\n        self.d_du = CuArray(zeros(ny,nx))\n\n        self.Δv = zeros(ny,nx)\n\n        return self\n    end\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Laplacian function remains unchanged. The main change to the explicit gating solvers is that *exp* and *expm1* functions are prefixed by *CUDAnative.*. This is a technical nuisance that will hopefully be resolved in future."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function rush_larsen_gpu(g, α, β, Δt)\n    inf = α/(α+β)\n    τ = 1.0/(α+β)\n    return clamp(g + (g - inf) * CUDAnative.expm1(-Δt/τ), 0f0, 1f0)\nend\n\nfunction update_M_gpu(g, v, Δt)\n    # the condition is needed here to prevent NaN when v == 47.0\n    α = isapprox(v, 47.0f0) ? 10.0f0 : -(v+47.0f0) / (CUDAnative.exp(-0.1f0*(v+47.0f0)) - 1.0f0)\n    β = (40.0f0 * CUDAnative.exp(-0.056f0*(v+72.0f0)))\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_H_gpu(g, v, Δt)\n    α = 0.126f0 * CUDAnative.exp(-0.25f0*(v+77.0f0))\n    β = 1.7f0 / (CUDAnative.exp(-0.082f0*(v+22.5f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_J_gpu(g, v, Δt)\n    α = (0.55f0 * CUDAnative.exp(-0.25f0*(v+78.0f0))) / (CUDAnative.exp(-0.2f0*(v+78.0f0)) + 1.0f0)\n    β = 0.3f0 / (CUDAnative.exp(-0.1f0*(v+32.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_D_gpu(g, v, Δt)\n    α = γ * (0.095f0 * CUDAnative.exp(-0.01f0*(v-5.0f0))) / (CUDAnative.exp(-0.072f0*(v-5.0f0)) + 1.0f0)\n    β = γ * (0.07f0 * CUDAnative.exp(-0.017f0*(v+44.0f0))) / (CUDAnative.exp(0.05f0*(v+44.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_F_gpu(g, v, Δt)\n    α = γ * (0.012f0 * CUDAnative.exp(-0.008f0*(v+28.0f0))) / (CUDAnative.exp(0.15f0*(v+28.0f0)) + 1.0f0)\n    β = γ * (0.0065f0 * CUDAnative.exp(-0.02f0*(v+30.0f0))) / (CUDAnative.exp(-0.2f0*(v+30.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_XI_gpu(g, v, Δt)\n    α = (0.0005f0 * CUDAnative.exp(0.083f0*(v+50.0f0))) / (CUDAnative.exp(0.057f0*(v+50.0f0)) + 1.0f0)\n    β = (0.0013f0 * CUDAnative.exp(-0.06f0*(v+20.0f0))) / (CUDAnative.exp(-0.04f0*(v+20.0f0)) + 1.0f0)\n    return rush_larsen_gpu(g, α, β, Δt)\nend\n\nfunction update_C_gpu(c, d, f, v, Δt)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * CUDAnative.log(c)\n    kCa = C_s * g_s * d * f\n    iCa = kCa * (v - ECa)\n    inf = 1.0f-7 * (0.07f0 - c)\n    τ = 1f0 / 0.07f0\n    return c + (c - inf) * CUDAnative.expm1(-Δt/τ)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, we modify the functions to calculate the individual currents by adding CUDAnative prefix."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "# iK1 is the inward-rectifying potassium current\nfunction calc_iK1(v)\n    ea = CUDAnative.exp(0.04f0*(v+85f0))\n    eb = CUDAnative.exp(0.08f0*(v+53f0))\n    ec = CUDAnative.exp(0.04f0*(v+53f0))\n    ed = CUDAnative.exp(-0.04f0*(v+23f0))\n    return 0.35f0 * (4f0*(ea-1f0)/(eb + ec)\n            + 0.2f0 * (isapprox(v, -23f0) ? 25f0 : (v+23f0) / (1f0-ed)))\nend\n\n# ix1 is the time-independent background potassium current\nfunction calc_ix1(v, xi)\n    ea = CUDAnative.exp(0.04f0*(v+77f0))\n    eb = CUDAnative.exp(0.04f0*(v+35f0))\n    return xi * 0.8f0 * (ea-1f0) / eb\nend\n\n# iNa is the sodium current (similar to the classic Hodgkin-Huxley model)\nfunction calc_iNa(v, m, h, j)\n    return C_Na * (g_Na * m^3 * h * j + g_NaC) * (v - ENa)\nend\n\n# iCa is the calcium current\nfunction calc_iCa(v, d, f, c)\n    ECa = D_Ca - 82.3f0 - 13.0278f0 * CUDAnative.log(c)    # ECa is the calcium reversal potential\n    return C_s * g_s * d * f * (v - ECa)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CUDA Kernels\n\nA CUDA program does not directly deal with GPCs and SMs. The logical view of a CUDA program is in the term of *blocks* and *threads*. We have to specify the number of block and threads when running a CUDA *kernel*. Each thread runs on a single CUDA core. Threads are logically bundled into blocks, which are in turn specified on a grid. The grid stands for the entirety of the domain of interest.\n\nEach thread can find its logical coordinate by using few pre-defined indexing variables (*threadIdx*, *blockIdx*, *blockDim* and *gridDim*) in C/C++ and the corresponding functions (e.g., `threadIdx()`) in Julia. There variables and functions are defined automatically for each thread and may return a different value depending on the calling thread. The return value of these functions is a 1, 2, or 3 dimensional structure whose elements can be accessed as `.x`, `.y`, and `.z` (for a 1-dimensional case, `.x` reports the actual index and `.y` and `.z` simply return 1). For example, if we deploy a kernel in 128 blocks and with 256 threads per block, each thread will see\n\n```\n    gridDim.x = 128;\n    blockDim=256;\n```\n\nwhile `blockIdx.x` ranges from 0 to 127 in C/C++ and 1 to 128 in Julia. Similarly, `threadIdx.x` will be between 0 to 255 in C/C++ (of course, in Julia the range will be 1 to 256).\n\nA C/C++ thread can calculate its index as\n\n```\n    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n```\n\nIn Julia, we have to take into account base 1. Therefore, we use the following formula\n\n```\n    idx = (blockIdx().x-UInt32(1)) * blockDim().x + threadIdx().x\n```\n\nA CUDA programmer is free to interpret the calculated index however it fits the application, but in practice, it is usually interpreted as an index into input tensors.\n\nIn the GPU version of the solver, each thread works on a single element of the medium, indexed by a (x,y) pair.\n`update_gates_gpu` and `update_du_gpu` are very similar to their CPU counterparts but are in fact CUDA kernels where the *for* loops are replaced with CUDA specific indexing. Note that CUDA kernels cannot return a valve; hence, *nothing* at the end."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function update_gates_gpu(u, XI, M, H, J, D, F, C, Δt)\n    i = (blockIdx().x-UInt32(1)) * blockDim().x + threadIdx().x\n    j = (blockIdx().y-UInt32(1)) * blockDim().y + threadIdx().y\n\n    v = Float32(u[i,j])\n\n    let Δt = Float32(Δt)\n        XI[i,j] = update_XI_gpu(XI[i,j], v, Δt)\n        M[i,j] = update_M_gpu(M[i,j], v, Δt)\n        H[i,j] = update_H_gpu(H[i,j], v, Δt)\n        J[i,j] = update_J_gpu(J[i,j], v, Δt)\n        D[i,j] = update_D_gpu(D[i,j], v, Δt)\n        F[i,j] = update_F_gpu(F[i,j], v, Δt)\n\n        C[i,j] = update_C_gpu(C[i,j], D[i,j], F[i,j], v, Δt)\n    end\n    nothing\nend\n\nfunction update_du_gpu(du, u, XI, M, H, J, D, F, C)\n    i = (blockIdx().x-UInt32(1)) * blockDim().x + threadIdx().x\n    j = (blockIdx().y-UInt32(1)) * blockDim().y + threadIdx().y\n\n    v = Float32(u[i,j])\n\n    # calculating individual currents\n    iK1 = calc_iK1(v)\n    ix1 = calc_ix1(v, XI[i,j])\n    iNa = calc_iNa(v, M[i,j], H[i,j], J[i,j])\n    iCa = calc_iCa(v, D[i,j], F[i,j], C[i,j])\n\n    # total current\n    I_sum = iK1 + ix1 + iNa + iCa\n\n    # the reaction part of the reaction-diffusion equation\n    du[i,j] = -I_sum / C_m\n    nothing\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implicit Solver\n\nFinally, the deriv function is modified to copy *u* to GPU and copy *du* back and to invoke CUDA kernels."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function (f::BeelerReuterGpu)(du, u, p, t)\n    L = 16   # block size\n    Δt = t - f.t\n    copyto!(f.d_u, u)\n    ny, nx = size(u)\n\n    if Δt != 0 || t == 0\n        @cuda blocks=(ny÷L,nx÷L) threads=(L,L) update_gates_gpu(\n            f.d_u, f.d_XI, f.d_M, f.d_H, f.d_J, f.d_D, f.d_F, f.d_C, Δt)\n        f.t = t\n    end\n\n    laplacian(f.Δv, u)\n\n    # calculate the reaction portion\n    @cuda blocks=(ny÷L,nx÷L) threads=(L,L) update_du_gpu(\n        f.d_du, f.d_u, f.d_XI, f.d_M, f.d_H, f.d_J, f.d_D, f.d_F, f.d_C)\n\n    copyto!(du, f.d_du)\n\n    # ...add the diffusion portion\n    du .+= f.diff_coef .* f.Δv\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ready to test!"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DifferentialEquations, Sundials\n\nderiv_gpu = BeelerReuterGpu(u0, 1.0);\nprob = ODEProblem(deriv_gpu, u0, (0.0, 50.0));\n@time sol = solve(prob, CVODE_BDF(linear_solver=:GMRES), saveat=100.0);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "heatmap(sol.u[end])"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n\nWe achieve around a 6x speedup with running the explicit portion of our IMEX solver on a GPU. The major bottleneck of this technique is the communication between CPU and GPU. In its current form, not all of the internals of the method utilize GPU acceleration. In particular, the implicit equations solved by GMRES are performed on the CPU. This partial CPU nature also increases the amount of data transfer that is required between the GPU and CPU (performed every f call). Compiling the full ODE solver to the GPU would solve both of these issues and potentially give a much larger speedup. [JuliaDiffEq developers are currently working on solutions to alleviate these issues](http://www.stochasticlifestyle.com/solving-systems-stochastic-pdes-using-gpus-julia/), but these will only be compatible with native Julia solvers (and not Sundials)."
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.1.0"
    },
    "kernelspec": {
      "name": "julia-1.1",
      "display_name": "Julia 1.1.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
