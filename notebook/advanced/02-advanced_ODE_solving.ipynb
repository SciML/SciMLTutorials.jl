{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Solving Stiff Equations\n### Chris Rackauckas\n\nThis tutorial is for getting into the extra features for solving stiff ordinary\ndifferential equations in an efficient manner. Solving stiff ordinary\ndifferential equations requires specializing the linear solver on properties of\nthe Jacobian in order to cut down on the O(n^3) linear solve and the O(n^2)\nback-solves. Note that these same functions and controls also extend to stiff\nSDEs, DDEs, DAEs, etc.\n\n## Code Optimization for Differential Equations\n\n### Writing Efficient Code\n\nFor a detailed tutorial on how to optimize one's DifferentialEquations.jl code,\nplease see the\n[Optimizing DiffEq Code tutorial](http://tutorials.juliadiffeq.org/html/introduction/03-optimizing_diffeq_code.html).\n\n### Choosing a Good Solver\n\nChoosing a good solver is required for getting top notch speed. General\nrecommendations can be found on the solver page (for example, the\n[ODE Solver Recommendations](http://docs.juliadiffeq.org/latest/solvers/ode_solve.html)).\nThe current recommendations can be simplified to a Rosenbrock method\n(`Rosenbrock23` or `Rodas5`) for smaller (<50 ODEs) problems, ESDIRK methods\nfor slightly larger (`TRBDF2` or `KenCarp4` for <2000 ODEs), and Sundials\n`CVODE_BDF` for even larger problems. `lsoda` from\n[LSODA.jl](https://github.com/rveltz/LSODA.jl) is generally worth a try.\n\nMore details on the solver to choose can be found by benchmarking. See the\n[DiffEqBenchmarks](https://github.com/JuliaDiffEq/DiffEqBenchmarks.jl) to\ncompare many solvers on many problems.\n\n### Check Out the Speed FAQ\n\nSee [this FAQ](http://docs.juliadiffeq.org/latest/basics/faq.html#Performance-1)\nfor information on common pitfalls and how to improve performance.\n\n### Setting Up Your Julia Installation for Speed\n\nJulia uses an underlying BLAS implementation for its matrix multiplications\nand factorizations. This library is automatically multithreaded and accelerates\nthe internal linear algebra of DifferentialEquations.jl. However, for optimality,\nyou should make sure that the number of BLAS threads that you are using matches\nthe number of physical cores and not the number of logical cores. See\n[this issue for more details](https://github.com/JuliaLang/julia/issues/33409).\n\nTo check the number of BLAS threads, use:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "ccall((:openblas_get_num_threads64_, Base.libblas_name), Cint, ())"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "If I want to set this directly to 4 threads, I would use:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using LinearAlgebra\nLinearAlgebra.BLAS.set_num_threads(4)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, in some cases Intel's MKL might be a faster BLAS than the standard\nBLAS that ships with Julia (OpenBLAS). To switch your BLAS implementation, you\ncan use [MKL.jl](https://github.com/JuliaComputing/MKL.jl) which will accelerate\nthe linear algebra routines. Please see the package for the limitations.\n\n### Use Accelerator Hardware\n\nWhen possible, use GPUs. If your ODE system is small and you need to solve it\nwith very many different parameters, see the\n[ensembles interface](http://docs.juliadiffeq.org/latest/features/ensemble.html)\nand [DiffEqGPU.jl](https://github.com/JuliaDiffEq/DiffEqGPU.jl). If your problem\nis large, consider using a [CuArray](https://github.com/JuliaGPU/CuArrays.jl)\nfor the state to allow for GPU-parallelism of the internal linear algebra.\n\n## Speeding Up Jacobian Calculations\n\nWhen one is using an implicit or semi-implicit differential equation solver,\nthe Jacobian must be built at many iterations and this can be one of the most\nexpensive steps. There are two pieces that must be optimized in order to reach\nmaximal efficiency when solving stiff equations: the sparsity pattern and the\nconstruction of the Jacobian. The construction is filling the matrix\n`J` with values, while the sparsity pattern is what `J` to use.\n\nThe sparsity pattern is given by a prototype matrix, the `jac_prototype`, which\nwill be copied to be used as `J`. The default is for `J` to be a `Matrix`,\ni.e. a dense matrix. However, if you know the sparsity of your problem, then\nyou can pass a different matrix type. For example, a `SparseMatrixCSC` will\ngive a sparse matrix. Additionally, structured matrix types like `Tridiagonal`,\n`BandedMatrix` (from\n[BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl)),\n`BlockBandedMatrix` (from\n[BlockBandedMatrices.jl](https://github.com/JuliaMatrices/BlockBandedMatrices.jl)),\nand more can be given. DifferentialEquations.jl will internally use this matrix\ntype, making the factorizations faster by utilizing the specialized forms.\n\nFor the construction, there are 3 ways to fill `J`:\n\n- The default, which uses normal finite/automatic differentiation\n- A function `jac(J,u,p,t)` which directly computes the values of `J`\n- A `colorvec` which defines a sparse differentiation scheme.\n\nWe will now showcase how to make use of this functionality with growing complexity.\n\n### Declaring Jacobian Functions\n\nLet's solve the Rosenbrock equations:\n\n$$\\begin{align}\ndy_1 &= -0.04y₁ + 10^4 y_2 y_3 \\\\\ndy_2 &= 0.04 y_1 - 10^4 y_2 y_3 - 3*10^7 y_{2}^2 \\\\\ndy_3 &= 3*10^7 y_{3}^2 \\\\\n\\end{align}$$\n\nIn order to reduce the Jacobian construction cost, one can describe a Jacobian\nfunction by using the `jac` argument for the `ODEFunction`. First, let's do\na standard `ODEProblem`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DifferentialEquations\nfunction rober(du,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  du[1] = -k₁*y₁+k₃*y₂*y₃\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  du[3] =  k₂*y₂^2\n  nothing\nend\nprob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\nsol = solve(prob,Rosenbrock23())\n\nusing Plots\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BenchmarkTools\n@btime solve(prob)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to add the Jacobian. First we have to derive the Jacobian\n$\\frac{df_i}{du_j}$ which is `J[i,j]`. From this we get:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function rober_jac(J,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  J[1,1] = k₁ * -1\n  J[2,1] = k₁\n  J[3,1] = 0\n  J[1,2] = y₃ * k₃\n  J[2,2] = y₂ * k₂ * -2 + y₃ * k₃ * -1\n  J[3,2] = y₂ * 2 * k₂\n  J[1,3] = k₃ * y₂\n  J[2,3] = k₃ * y₂ * -1\n  J[3,3] = 0\n  nothing\nend\nf = ODEFunction(rober, jac=rober_jac)\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\n\n@btime solve(prob_jac)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Derivation of Jacobian Functions\n\nBut that was hard! If you want to take the symbolic Jacobian of numerical\ncode, we can make use of [ModelingToolkit.jl](https://github.com/JuliaDiffEq/ModelingToolkit.jl)\nto symbolicify the numerical code and do the symbolic calculation and return\nthe Julia code for this."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using ModelingToolkit\nde = modelingtoolkitize(prob)\nModelingToolkit.generate_jacobian(de...)[2] # Second is in-place"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "which outputs:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        ":((##MTIIPVar#376, u, p, t)->begin\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:65 =#\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:66 =#\n          let (x₁, x₂, x₃, α₁, α₂, α₃) = (u[1], u[2], u[3], p[1], p[2], p[3])\n              ##MTIIPVar#376[1] = α₁ * -1\n              ##MTIIPVar#376[2] = α₁\n              ##MTIIPVar#376[3] = 0\n              ##MTIIPVar#376[4] = x₃ * α₃\n              ##MTIIPVar#376[5] = x₂ * α₂ * -2 + x₃ * α₃ * -1\n              ##MTIIPVar#376[6] = x₂ * 2 * α₂\n              ##MTIIPVar#376[7] = α₃ * x₂\n              ##MTIIPVar#376[8] = α₃ * x₂ * -1\n              ##MTIIPVar#376[9] = 0\n          end\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:67 =#\n          nothing\n      end)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's use that to give the analytical solution Jacobian:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "jac = eval(ModelingToolkit.generate_jacobian(de...)[2])\nf = ODEFunction(rober, jac=jac)\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declaring a Sparse Jacobian\n\nJacobian sparsity is declared by the `jac_prototype` argument in the `ODEFunction`.\nNote that you should only do this if the sparsity is high, for example, 0.1%\nof the matrix is non-zeros, otherwise the overhead of sparse matrices can be higher\nthan the gains from sparse differentiation!\n\nBut as a demonstration, let's build a sparse matrix for the Rober problem. We\ncan do this by gathering the `I` and `J` pairs for the non-zero components, like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "I = [1,2,1,2,3,1,2]\nJ = [1,1,2,2,2,3,3]\nusing SparseArrays\njac_prototype = sparse(I,J,1.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now this is the sparse matrix prototype that we want to use in our solver, which\nwe then pass like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f = ODEFunction(rober, jac=jac, jac_prototype=jac_prototype)\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Automatic Sparsity Detection\n\nOne of the useful companion tools for DifferentialEquations.jl is\n[SparsityDetection.jl](https://github.com/JuliaDiffEq/SparsityDetection.jl).\nThis allows for automatic declaration of Jacobian sparsity types. To see this\nin action, let's look at the 2-dimensional Brusselator equation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "const N = 32\nconst xyd_brusselator = range(0,stop=1,length=N)\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\nlimit(a, N) = a == N+1 ? 1 : a == 0 ? N : a\nfunction brusselator_2d_loop(du, u, p, t)\n  A, B, alpha, dx = p\n  alpha = alpha/dx^2\n  @inbounds for I in CartesianIndices((N, N))\n    i, j = Tuple(I)\n    x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\n    ip1, im1, jp1, jm1 = limit(i+1, N), limit(i-1, N), limit(j+1, N), limit(j-1, N)\n    du[i,j,1] = alpha*(u[im1,j,1] + u[ip1,j,1] + u[i,jp1,1] + u[i,jm1,1] - 4u[i,j,1]) +\n                B + u[i,j,1]^2*u[i,j,2] - (A + 1)*u[i,j,1] + brusselator_f(x, y, t)\n    du[i,j,2] = alpha*(u[im1,j,2] + u[ip1,j,2] + u[i,jp1,2] + u[i,jm1,2] - 4u[i,j,2]) +\n                A*u[i,j,1] - u[i,j,1]^2*u[i,j,2]\n    end\nend\np = (3.4, 1., 10., step(xyd_brusselator))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this setup, we can give and example `input` and `output` and call `sparsity!`\non our function with the example arguments and it will kick out a sparse matrix\nwith our pattern, that we can turn into our `jac_prototype`."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SparsityDetection, SparseArrays\ninput = rand(32,32,2)\noutput = similar(input)\nsparsity_pattern = sparsity!(brusselator_2d_loop,output,input,p,0.0)\njac_sparsity = Float64.(sparse(sparsity_pattern))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's double check what our sparsity pattern looks like:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nspy(jac_sparsity,markersize=1,colorbar=false,color=:deep)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's neat, and would be tedius to build by hand! Now we just pass it to the\n`ODEFunction` like as before:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f = ODEFunction(brusselator_2d_loop;jac_prototype=jac_sparsity)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the `ODEProblem`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function init_brusselator_2d(xyd)\n  N = length(xyd)\n  u = zeros(N, N, 2)\n  for I in CartesianIndices((N, N))\n    x = xyd[I[1]]\n    y = xyd[I[2]]\n    u[I,1] = 22*(y*(1-y))^(3/2)\n    u[I,2] = 27*(x*(1-x))^(3/2)\n  end\n  u\nend\nu0 = init_brusselator_2d(xyd_brusselator)\nprob_ode_brusselator_2d = ODEProblem(brusselator_2d_loop,\n                                     u0,(0.,11.5),p)\n\nprob_ode_brusselator_2d_sparse = ODEProblem(f,\n                                     u0,(0.,11.5),p)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how the version with sparsity compares to the version without:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime solve(prob_ode_brusselator_2d,save_everystep=false)\n@btime solve(prob_ode_brusselator_2d_sparse,save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Declaring Color Vectors for Fast Construction\n\nIf you cannot directly define a Jacobian function, you can use the `colorvec`\nto speed up the Jacobian construction. What the `colorvec` does is allows for\ncalculating multiple columns of a Jacobian simultaniously by using the sparsity\npattern. An explanation of matrix coloring can be found in the\n[MIT 18.337 Lecture Notes](https://mitmath.github.io/18337/lecture9/stiff_odes).\n\nTo perform general matrix coloring, we can use\n[SparseDiffTools.jl](https://github.com/JuliaDiffEq/SparseDiffTools.jl). For\nexample, for the Brusselator equation:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using SparseDiffTools\ncolorvec = matrix_colors(jac_sparsity)\n@show maximum(colorvec)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This means that we can now calculate the Jacobian in 12 function calls. This is\na nice reduction from 2048 using only automated tooling! To now make use of this\ninside of the ODE solver, you simply need to declare the colorvec:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f = ODEFunction(brusselator_2d_loop;jac_prototype=jac_sparsity,\n                                    colorvec=colorvec)\nprob_ode_brusselator_2d_sparse = ODEProblem(f,\n                                     init_brusselator_2d(xyd_brusselator),\n                                     (0.,11.5),p)\n@btime solve(prob_ode_brusselator_2d_sparse,save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the massive speed enhancement!\n\n## Defining Linear Solver Routines and Jacobian-Free Newton-Krylov\n\nA completely different way to optimize the linear solvers for large sparse\nmatrices is to use a Krylov subpsace method. This requires choosing a linear\nsolver for changing to a Krylov method. Optionally, one can use a Jacobian-free\noperator to reduce the memory requirements.\n\n### Declaring a Jacobian-Free Newton-Krylov Implementation\n\nTo swap the linear solver out, we use the `linsolve` command and choose the\nGMRES linear solver."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@btime solve(prob_ode_brusselator_2d,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false)\n@btime solve(prob_ode_brusselator_2d_sparse,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information on linear solver choices, see the\n[linear solver documentation](http://docs.juliadiffeq.org/latest/features/linear_nonlinear.html).\n\nOn this problem, handling the sparsity correctly seemed to give much more of a\nspeedup than going to a Krylov approach, but that can be dependent on the problem\n(and whether a good preconditioner is found).\n\nWe can also enhance this by using a Jacobian-Free implementation of `f'(x)*v`.\nTo define the Jacobian-Free operator, we can use\n[DiffEqOperators.jl](https://github.com/JuliaDiffEq/DiffEqOperators.jl) to generate\nan operator `JacVecOperator` such that `Jv*v` performs `f'(x)*v` without building\nthe Jacobian matrix."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DiffEqOperators\nJv = JacVecOperator(brusselator_2d_loop,u0,p,0.0)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "and then we can use this by making it our `jac_prototype`:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "f = ODEFunction(brusselator_2d_loop;jac_prototype=Jv)\nprob_ode_brusselator_2d_jacfree = ODEProblem(f,u0,(0.,11.5),p)\n@btime solve(prob_ode_brusselator_2d_jacfree,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding a Preconditioner\n\nThe [linear solver documentation](http://docs.juliadiffeq.org/latest/features/linear_nonlinear.html#IterativeSolvers.jl-Based-Methods-1)\nshows how you can add a preconditioner to the GMRES. For example, you can\nuse packages like [AlgebraicMultigrid.jl](https://github.com/JuliaLinearAlgebra/AlgebraicMultigrid.jl)\nto add an algebraic multigrid (AMG) or [IncompleteLU.jl](https://github.com/haampie/IncompleteLU.jl)\nfor an incomplete LU-factorization (iLU)."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using AlgebraicMultigrid\npc = aspreconditioner(ruge_stuben(jac_sparsity))\n@btime solve(prob_ode_brusselator_2d_jacfree,TRBDF2(linsolve=LinSolveGMRES(Pl=pc)),save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Structured Matrix Types\n\nIf your sparsity pattern follows a specific structure, for example a banded\nmatrix, then you can declare `jac_prototype` to be of that structure and then\nadditional optimizations will come for free. Note that in this case, it is\nnot necessary to provide a `colorvec` since the color vector will be analytically\nderived from the structure of the matrix.\n\nThe matrices which are allowed are those which satisfy the\n[ArrayInterface.jl](https://github.com/JuliaDiffEq/ArrayInterface.jl) interface\nfor automatically-colorable matrices. These include:\n\n- Bidiagonal\n- Tridiagonal\n- SymTridiagonal\n- BandedMatrix ([BandedMatrices.jl](https://github.com/JuliaMatrices/BandedMatrices.jl))\n- BlockBandedMatrix ([BlockBandedMatrices.jl](https://github.com/JuliaMatrices/BlockBandedMatrices.jl))\n\nMatrices which do not satisfy this interface can still be used, but the matrix\ncoloring will not be automatic, and an appropriate linear solver may need to\nbe given (otherwise it will default to attempting an LU-decomposition).\n\n## Sundials-Specific Handling\n\nWhile much of the setup makes the transition to using Sundials automatic, there\nare some differences between the pure Julia implementations and the Sundials\nimplementations which must be taken note of. These are all detailed in the\n[Sundials solver documentation](http://docs.juliadiffeq.org/latest/solvers/ode_solve.html#Sundials.jl-1),\nbut here we will highlight the main details which one should make note of.\n\nDefining a sparse matrix and a Jacobian for Sundials works just like any other\npackage. The core difference is in the choice of the linear solver. With Sundials,\nthe linear solver choice is done with a Symbol in the `linear_solver` from a\npreset list. Particular choices of note are `:Band` for a banded matrix and\n`:GMRES` for using GMRES. If you are using Sundials, `:GMRES` will not require\ndefining the JacVecOperator, and instead will always make use of a Jacobian-Free\nNewton Krylov (with numerical differentiation). Thus on this problem we could do:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Sundials\n# Sparse Version\n@btime solve(prob_ode_brusselator_2d_sparse,CVODE_BDF(),save_everystep=false)\n# GMRES Version: Doesn't require any extra stuff!\n@btime solve(prob_ode_brusselator_2d,CVODE_BDF(linear_solver=:GMRES),save_everystep=false)"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Details for setting up a preconditioner with Sundials can be found at the\n[Sundials solver page](http://docs.juliadiffeq.org/latest/solvers/ode_solve.html#Sundials.jl-1).\n\n## Handling Mass Matrices\n\nInstead of just defining an ODE as $u' = f(u,p,t)$, it can be common to express\nthe differential equation in the form with a mass matrix:\n\n$$Mu' = f(u,p,t)$$\n\nwhere $M$ is known as the mass matrix. Let's solve the Robertson equation.\nAt the top we wrote this equation as:\n\n$$\\begin{align}\ndy_1 &= -0.04y₁ + 10^4 y_2 y_3 \\\\\ndy_2 &= 0.04 y_1 - 10^4 y_2 y_3 - 3*10^7 y_{2}^2 \\\\\ndy_3 &= 3*10^7 y_{3}^2 \\\\\n\\end{align}$$\n\nBut we can instead write this with a conservation relation:\n\n$$\\begin{align}\ndy_1 &= -0.04y₁ + 10^4 y_2 y_3 \\\\\ndy_2 &= 0.04 y_1 - 10^4 y_2 y_3 - 3*10^7 y_{2}^2 \\\\\n1 &=  y_{1} + y_{2} + y_{3} \\\\\n\\end{align}$$\n\nIn this form, we can write this as a mass matrix ODE where $M$ is singular\n(this is another form of a differential-algebraic equation (DAE)). Here, the\nlast row of `M` is just zero. We can implement this form as:"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using DifferentialEquations\nfunction rober(du,u,p,t)\n  y₁,y₂,y₃ = u\n  k₁,k₂,k₃ = p\n  du[1] = -k₁*y₁+k₃*y₂*y₃\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  du[3] =  y₁ + y₂ + y₃ - 1\n  nothing\nend\nM = [1. 0  0\n     0  1. 0\n     0  0  0]\nf = ODEFunction(rober,mass_matrix=M)\nprob_mm = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\nsol = solve(prob_mm,Rodas5())\n\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that if your mass matrix is singular, i.e. your system is a DAE, then you\nneed to make sure you choose\n[a solver that is compatible with DAEs](http://docs.juliadiffeq.org/latest/solvers/dae_solve.html#Full-List-of-Methods-1)"
      ],
      "metadata": {}
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.2.0"
    },
    "kernelspec": {
      "name": "julia-1.2",
      "display_name": "Julia 1.2.0",
      "language": "julia"
    }
  },
  "nbformat": 4
}
